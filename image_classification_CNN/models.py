import torch.nn as nn
import torch.nn.functional as F

class AlexNet(nn.Module):
    def __init__(self, dropout_prob= 0.5, num_classes=10):
        super(AlexNet, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(64, 192, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(192, 384, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(384, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )
        self.fc = nn.Sequential(
            nn.Linear(256 * 3 * 3, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=dropout_prob),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(p=dropout_prob),
            nn.Linear(4096, num_classes),
        )

    def forward(self, x):
        # 让x通过卷积部分
        x = self.conv(x)
        # 展平tensor
        x = x.view(x.size(0), -1)
        # 然后通过全连接部分
        x = self.fc(x)
        return x

class VGGNet(nn.Module):
    def __init__(self, dropout_prob= 0.5, num_classes=10):
        super(VGGNet, self).__init__()
        """
        TODO 1:
            请参考AlexNet的实现和给定的模型结构，在此处完善VGG11-variant的卷积和全连接部分
            nn.Dropout()的放置方式和AlexNet保持一致即可
        """
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size = 3, stride = 1, padding = 1),
            nn.ReLU(inplace = True),
            nn.MaxPool2d(kernel_size = 2, stride = 2),
            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),
            nn.ReLU(inplace = True),
            nn.MaxPool2d(kernel_size = 2, stride = 2),
            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),
            nn.ReLU(inplace = True),
            nn.Conv2d(256, 256, kernel_size = 3, stride = 1, padding = 1),
            nn.ReLU(inplace = True),
            nn.MaxPool2d(kernel_size = 2, stride = 2),
            nn.Conv2d(256, 512, kernel_size = 3, stride = 1, padding = 1),
            nn.ReLU(inplace = True),
            nn.Conv2d(512, 512, kernel_size = 3, stride = 1, padding = 1),
            nn.ReLU(inplace = True),
            nn.MaxPool2d(kernel_size = 2, stride = 2)
        )
        self.fc = nn.Sequential(
            nn.Linear(512 * 1 * 1, 4096),
            nn.ReLU(inplace = True),
            nn.Dropout(p = dropout_prob),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace = True),
            nn.Dropout(p = dropout_prob),
            nn.Linear(4096, num_classes)
        )

    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

class MSELoss(nn.Module):
    def __init__(self):
        super(MSELoss, self).__init__()
        self.l2 = nn.MSELoss()

    def forward(self, estimate, target):
        # 获取device确保target tensor和estimate tensor在同一device上
        device = estimate.device
        # 从estimate tensor获取类别的总数
        n = estimate.size(1)
        # 将target转换为one-hot编码形式
        target = F.one_hot(target, num_classes=n).float().to(device)
        # 计算MSE loss
        return self.l2(estimate, target)